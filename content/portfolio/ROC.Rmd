---
image: "img/portfolio/gravity-paper.jpg"
showonlyimage: false
date: "2016-11-05T19:44:32+05:30"
title: "ROC Curves"
description: "A short description perhaps?"
type: "portfolio"
draft: false
---

<style> /* set the CSS */
  circle.raspberry {
    fill: #d20000;
    stroke: #500000;
    stroke-width: 3px;
  }
  circle.blueberry {
    fill: #0059de;
    stroke: #002050;
    stroke-width: 3px;
  }
  circle.pos {
    fill: none;
    stroke: #d20000;
    stroke-opacity: 0.75;
    stroke-width: 5px;
    stroke-dasharray: 10;
  }
  circle.neg {
    fill: none;
    stroke: #0059de;
    stroke-opacity: 0.75;
    stroke-width: 5px;
    stroke-dasharray: 10;
  }
</style>

In this post we'll explore what [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves are, how they're constructed, and what they're useful for, along with some important limitations.

## The muffin machine

Imagine we've built a machine that bakes two kinds of muffins, blueberry and raspberry. The only thing we need to do is feed the machine raw ingredients, and within thirty minutes, fresh muffins pop out the other end.

Suppose that we buy our berries in bulk and that they come mixed together. Because we're lazy (and hungry), we want the machine to seperate the rapsberries from the blueberries for us. This is an example of a [classification problem](https://en.wikipedia.org/wiki/Statistical_classification).


<img src="/img/roc/muffin-machine.png" style = "max-width:300px;width:100%;"/>


After developing a classification algorithm that the machine will use to sort the mixed berries, we'd like to estimate how well it's going to perform. An ROC curve is one tool that can help us do this. Here's an example curve which we'll unpack below.

<figure class="roc-example" style = "max-width:330px;width:100%"></figure> 


There's a lot of information behind this plot, and it's important to understand what the ROC curve can and can't do. 

## Confusion matrix

To understand how the ROC curve is constructed, it helps knowing about confusion matrices. Because we only have two types of berries, we can conceptualise our classifier as one that only recognises things as "raspberry" and "not raspberry" (this is *binary* classification). In this way, we'll label raspberries "positive" and blueberries "negative". 

!image of blueberry and raspberry!

<!-- It would actually be completely fine to not introduce the *positive* / *negative* language and just stick with berries, but I'm hoping it will help highlight how all of this generalises.  -->

<!-- To keep things simple, we'll keep representing berries as simple circles (totally not a cop out to save me from putting more effort into drawing things) -->

<!-- <figure class = "berries" style = "max-width:300px; width:100%"></figure> -->

Given this definition, there are four possible ways our classifier can label berries.
Here, we add a dotted line around the berries to indicate our classifier's prediction of the berry type:
<!-- <figure class = "berryLabels" style = "max-width:200px; width:100%"></figure> -->

<figure class = "berryLabels" style = "max-width:170px"></figure>

<!-- < image of blueberries and raspberries with labels (2 x 2 grid) > -->

<!-- We'd call a raspberry that's correctly labelled a *true positive*, and a correctly labelled blueberry a *true negative*. A blueberry incorrectly labelled "raspberry" is a *false positive*, and a raspberry incorrectly "not raspberry" would be a *false negative*. -->

We use the following terminology to refer to each of the possibilities:

- *true positive* - a correctly labelled raspberry (top left)
- *true negative* - a correctly labelled blueberry (bottom right)
- *false positive* - a blueberry incorrectly labelled "raspberry" (bottom left)
- *false negative* - a raspberry incorrectly labelled "blueberry" (top right)

A confusion matrix is essentially a way to visualise the correctness of a set of labels by counting the numbers in each category.
Change the labels of these berries by clicking on them to see how this works:

<figure class = "berryClasses" style = "max-width:400px"></figure>
<figure class = "berryMatrix" style = "max-width:250px"></figure>

There are some standard terms that describe different ratios of values from the confusion matrix, for example: 

<figure class = "berryClasses2" style = "max-width:420px"></figure>
<figure class = "berryMatrix" style = "max-width:250px"></figure>

- True positive rate, or *sensitivity* = $\frac{tp}{p}$ = <span id = "sensitivity"></span>
- True negative rate, or *specificity*  = $\frac{tn}{n}$ = <span id = "specificity"></span>
- Positive predicted value, or *precision*  = $\frac{tp}{tp + fp}$ = <span id = "precision"></span>

â€¦and a [whole bunch of others](https://en.wikipedia.org/wiki/Confusion_matrix)



### Points in the ROC space

Any set of predictions where the actual class is known can be shown on an ROC plot as a point -- the x axis is simply the false positive rate, and the y axis is the true positive rate. This is a nice visual way to summarise one part of the confusion matrix. Below we're showing the current arrangement of berry predictions from before.

<figure class = "berryClasses" style = "max-width:320px"></figure>

< consider adding a smaller version of the force layout so that they can be side by side >

< ROC space with point and grid >

Play around with the berries again and try move the point to each corner of the plot. 
Some key ideas to think about:

- The upper right corner corresponds to labelling everything "positive".
- The lower left corner corresponds to labelling everything "negative".
- The upper left corner corresponds to perfect prediction.

And equally important, but a bit trickier:

- A set of predictions that lies anywhere on the diagonal line is doing no better at seperating berries than random guessing.
- Any point that lies below the diagonal line is doing worse than guessing, but can produce useful results by simply reversing the predicted labels; this causes the point to reflect about the diagonal line.
- The point can only occupy positions on an equally spaced grid, because when we have a finite number of berries, the true and false positive rates are always a fraction of integers with a fixed denominator.



### Discrimination threshold

So far we've discussed our classifier's predictions in categorical terms -- a berry is either a blueberry or a raspberry. 
Most algorithms that can be used for classification actually give you more information than just a label.
For example, logistic regression, support vector machines, and random forests will all output a continuous value which is *related* to the probability that the input belongs to a certain class. 
(I say *related to probability* with care because the raw output is not necessarily a true probability, more on this later).

Applied to our example, our classifier may output a value of "0.1" when it's quite sure that it's looking at a blueberry, and a value of "0.99" when it's nearly certain it has a raspberry. 
If we applied our algorithm to a sample of berries, we might get a set of outputs that look like this:

< figure that shows how our classifier places berries on a continuous scale >

So how can we take this continuous output and turn it into categorical predictions? 
One simplistic approach is to apply a threshold or cutoff. 
Predictions below the cutoff are labelled negative, and predictions above are labelled positive.

< figure that shows the threshold and how it dictates predictions >
< throw in a confusion matrix? would be neat >

Setting a threshold is common practice, but it can be a naive approach that doesn't make full use of the available information. An alternative will be briefly discussed later.

## Drawing the ROC curve

If you've played around with the interactive threshold above, hopefully you can see the trade-offs that exist in classification. 
To increase the true positive rate, you could set a low threshold, but this usually comes at a cost of more false positives. 

The ROC curve is a visualisation of this trade-off and the performance of the classifier across all thresholds. Here's the same threshold slider, but this time we show the ROC plot. Notice how every threshold value corresponds to a point on the curve.

< same threshold slider as before, but this time show the ROC plot and the current threshold as a point along the stepped line > 


### Using base R

There are packages that can give us the ROC plot of a classifier for free, but it's useful to think about how to do it yourself.



I've seen two algorithms for drawing the curve -- they both work, but I think one is clearly better.
We'll focus on generating the coordinates of points that make up the ROC curve, and then use ggplot2 at the end to actually draw it. 

To demonstrate, we'll use R and take an excursion from the muffin machine. Here 1 is used for positives and 0 for negatives:

```{r, echo = FALSE, message = FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
```


```{r}
# Let's make up some dummy predictions
pr <- tibble::tribble(
    ~prediction, ~actual,
    0.9,           1, 
    0.84,          1,
    0.7,           0, 
    0.55,          1,
    0.24,          0, 
    0.11,          0,
    0.08,          1
  )
```

The inferior algorithm usually looks something like this:

```{r, fig.width = 3, fig.height = 3}
step <- 0.01 
rocPoints <- data.frame(threshold = seq(0, 1 + step, step),
                        fpr = NA, 
                        tpr = NA)
nPos = sum(pr$actual) 
nNeg = nrow(pr) - nPos 

for (i in seq_along(rocPoints$threshold)) {
  thr <-  1 + step - step * (i - 1)
  fp <- filter(pr, actual == 0, prediction >= thr) %>% count(.)
  tp <- filter(pr, actual == 1, prediction >= thr) %>% count(.)
  rocPoints[i, "fpr"] <- fp / nNeg
  rocPoints[i, "tpr"] <- tp / nPos
}

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() + 
  geom_point()
```

The reason I don't like this is not that it's usually less efficient, or even that it requires a seemingly arbitrary small magic number *step*, but because I think it reflects a lack of understanding of what the ROC curve is measuring (spoiler alert -- it's discrimination performance).

A superior approach becomes clear once you understand that the predicted *values* are irrelevant; it's only the *ordering* of predictions that matter for the ROC curve. Here's the better algorithm:

```{r, fig.width = 3, fig.height = 2}

pr <- arrange(pr, desc(prediction))

rocPoints <- data.frame(fpr = cumsum(!pr$actual) / sum(!pr$actual),
                        tpr = cumsum(pr$actual) / sum(pr$actual))

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() + 
  geom_point() +
  coord_fixed()
```

```{r, fig.width = 3, fig.height = 2}

pr <- arrange(pr, desc(prediction))

rocPoints <- data.frame(fpr = cumsum(!pr$actual) / sum(!pr$actual),
                        tpr = cumsum(pr$actual) / sum(pr$actual))

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() + 
  geom_point() 
```


## Area under the curve (AUC)

### Discrimination performance

- introduce slider! Watch ROC curve change.
- emphasise how only *ordering* is important.

### Discrimination vs. calibration

- show extreme examples with all berries bunched up at one end of the scale.
- consider animation where berries jiggle arround, but don't swap places.
- Frank Harrel quote

## Optimal decision theory

- Mostly provide links to resources


<script src = "/js/d3.js"></script>
<script src = "/js/berry-clusters-new.js"></script>
<script src = "/js/confusion-matrix.js"></script>
<script src = "/js/ROC-visuals.js"></script>


<!-- <script src="https://d3js.org/d3.v4.min.js"></script> -->
<!-- <script src = "/js/roc2.js"></script> -->
<!-- <script src = "/js/berries.js"></script> -->
<!-- <script src = "/js/confusion-matrix.js"></script> -->

## Closing

< visualisation that ties everything together >
