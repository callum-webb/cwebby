---
image: "img/portfolio/gravity-paper.jpg"
showonlyimage: false
date: "2016-11-05T19:44:32+05:30"
title: "ROC Curves"
description: "A short description perhaps?"
type: "portfolio"
draft: false
---

<link rel="stylesheet" href="/css/roc.css" />

In this post we'll explore what [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves are, how they're constructed, and what they're useful for, along with some important limitations.

## The muffin machine

Imagine we've built a machine that bakes two kinds of muffins, blueberry and raspberry. The only thing we need to do is feed the machine raw ingredients, and within thirty minutes, fresh muffins pop out the other end.

Suppose that we buy our berries in bulk and that they come mixed together. Because we're lazy (and hungry), we want the machine to seperate the rapsberries from the blueberries for us. This is an example of a [classification problem](https://en.wikipedia.org/wiki/Statistical_classification).


<img src="/img/roc/muffin-machine.png" style = "max-width:300px;width:100%;"/>


After developing a classification algorithm that the machine will use to sort the mixed berries, we'd like to estimate how well it's going to perform. An ROC curve is one tool that can help us do this. Here's an example curve which we'll unpack below.

<figure class="roc-example" style = "max-width:330px;width:100%"></figure>


There's a lot of information behind this plot, and it's important to understand what the ROC curve can and can't do.

## Confusion matrix

To understand how the ROC curve is constructed, it helps knowing about confusion matrices. Because we only have two types of berries, we can conceptualise our classifier as one that only recognises things as "raspberry" and "not raspberry" (this is *binary* classification). In this way, we'll label raspberries "positive" and blueberries "negative".

<!-- It would actually be completely fine to not introduce the *positive* / *negative* language and just stick with berries, but I'm hoping it will help highlight how all of this generalises.  -->

<!-- To keep things simple, we'll keep representing berries as simple circles (totally not a cop out to save me from putting more effort into drawing things) -->

<figure class = "berries" style = "max-width:210px; width:100%"></figure>

Given this definition, there are four possible combinations of berries and labels our classifier can come up with.
Here, we add a dotted line around the berries to indicate our classifier's prediction of the berry type:

<figure class = "berryLabels" style = "max-width:120px"></figure>

We use the following terminology to refer to each of the possibilities:

- *true positive* - a correctly labelled raspberry (top left)
- *true negative* - a correctly labelled blueberry (bottom right)
- *false positive* - a blueberry incorrectly labelled "raspberry" (bottom left)
- *false negative* - a raspberry incorrectly labelled "blueberry" (top right)

A confusion matrix is essentially a way to visualise the correctness of a set of labels by counting the numbers in each category.
Here's an example; it's interactive, so click on the berries underneath the matrix to see how this works:

<figure class = "berryMatrix" style = "max-width:220px"></figure>
<hr/>
<figure class = "berryClasses" style = "max-width:400px"></figure>

There are some standard terms that describe different ratios of values from the confusion matrix, for example:

- True positive rate, or *sensitivity* = $\frac{tp}{p}$ = <span id = "sensitivity"></span>
- True negative rate, or *specificity*  = $\frac{tn}{n}$ = <span id = "specificity"></span>
- Positive predicted value, or *precision*  = $\frac{tp}{tp + fp}$ = <span id = "precision"></span>

â€¦and a [whole bunch of others](https://en.wikipedia.org/wiki/Confusion_matrix)


### Points in the ROC space

Any set of predictions where the actual class is known can be shown on an ROC plot as a point -- the x axis is simply the false positive rate, and the y axis is the true positive rate. This is a nice visual way to summarise two aspects of the confusion matrix.

Below we're showing the current state of the labels as a point in the ROC space:

<figure class = "rocPoint" style = "max-width:270px"></figure>
<figure class = "berryClasses" style = "max-width:320px"></figure>

Play around with the berries again and try move the point to each corner of the plot.
Some key ideas to think about:

- The upper right corner corresponds to labelling everything "positive".
- The lower left corner corresponds to labelling everything "negative".
- The upper left corner corresponds to perfect prediction.

And equally important, but a bit trickier:

- A set of predictions that lies anywhere on the diagonal line is doing no better at seperating berries than random guessing. (This is why you will often see it drawn - it's a useful reference).
- Any point that lies below the diagonal line is doing worse than guessing, but can produce useful results by simply reversing the predicted labels; this causes the point to reflect about the diagonal line.
- The point can only occupy fixed positions a grid, because when we have a finite number of berries, the true and false positive rates are always a fraction of integers with a fixed denominator.



### Discrimination threshold

So far we've discussed our classifier's predictions in categorical terms -- a berry is either a blueberry or a raspberry.
Most algorithms that can be used for classification actually give you more information than just a label.
For example, logistic regression will output a continuous value which is *related* to the probability that the input belongs to a certain class.
(I say *related to probability* with care because the raw output is not necessarily a true probability, more on this later).

<figure class = "berryCont" style = "max-width:410px"></figure>

Applied to our example, our classifier may output a value of 0.99 when it's nearly certain it's looking at a raspberry, and a value of 0.1 when it's quite sure it has something that is not a raspberry (i.e. blueberry). If we applied our algorithm to a sample of twelve berries, six of each type, we might get a set of outputs that look like this:

<figure class = "berryContPop" style = "max-width:410px"></figure>

So how can we make categorical decisions based on continuous data like this?
One simplistic approach is to apply a threshold or cutoff.
Predictions below the cutoff are labelled negative, and predictions above are labelled positive.

<figure class = "discrimPlot" style = "max-width:410px"></figure>

Move the slider above to adjust the threshold and observe the results. Notice that if you want to correctly label all of the raspberries, you'll have to put up with three false positives. And if you need to ensure you have no false positives, then you must accept five false negatives. This is a reality of just about all real-world data.

As an aside, while setting a threshold is common practice, it can be a naive approach that doesn't make full use of the available information. You might also have not considered applying threshold values other than 0.5 or 50%. An alternative will be briefly discussed later.

## Drawing the ROC curve

If you've played around with the interactive threshold above, hopefully you can see the trade-offs that exist in classification.
To increase the true positive rate, you could set a low threshold, but this usually comes at a cost of more false positives.

The ROC curve is a visualisation of this trade-off and the performance of the classifier across all thresholds. Here's the same threshold slider, but this time we show the ROC plot. Notice how every threshold value corresponds to a point on the curve.

< same threshold slider as before, but this time show the ROC plot and the current threshold as a point along the stepped line >


### Doing it yourself

There are packages that can give us the ROC plot of a classifier for free, but it's useful to think about how you might do it yourself.

I've seen two algorithms for drawing the curve -- they both work, but I think one is clearly better.
We'll focus on generating the coordinates of points that make up the ROC curve, and then use ggplot2 at the end to actually draw it.

To demonstrate, suppose our muffin machine generates the following output when presented with four raspberries and three blueberries, where 1 indicates a raspberry (positive) and 0 indicates a blueberry (negative):

```{r, echo = FALSE, message = FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
```


```{r}
# Let's make up some dummy predictions
pr <- tibble::tribble(
    ~prediction, ~actual,
    0.9,           1,
    0.84,          1,
    0.7,           0,
    0.55,          1,
    0.24,          0,
    0.11,          0,
    0.08,          1
  )
```

The inferior algorithm usually looks something like this:

```{r, fig.width = 3, fig.height = 3}
step <- 0.01
rocPoints <- data.frame(threshold = seq(0, 1 + step, step),
                        fpr = NA,
                        tpr = NA)
nPos = sum(pr$actual)
nNeg = nrow(pr) - nPos

for (i in seq_along(rocPoints$threshold)) {
  thr <-  1 + step - step * (i - 1)
  fp <- filter(pr, actual == 0, prediction >= thr) %>% count(.)
  tp <- filter(pr, actual == 1, prediction >= thr) %>% count(.)
  rocPoints[i, "fpr"] <- fp / nNeg
  rocPoints[i, "tpr"] <- tp / nPos
}

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() +
  geom_point()
```

The reason I don't like this is not that it's usually less efficient, or even that it requires a seemingly arbitrary small magic number *step*, but because I think it reflects a lack of understanding of what the ROC curve is measuring (spoiler alert -- it's discrimination performance).

A superior approach becomes clear once you understand that the predicted *values* are irrelevant; it's only the *ordering* of predictions that matter for the ROC curve. Here's the better algorithm:

```{r, fig.width = 3, fig.height = 2}

pr <- arrange(pr, desc(prediction))

rocPoints <- data.frame(fpr = cumsum(!pr$actual) / sum(!pr$actual),
                        tpr = cumsum(pr$actual) / sum(pr$actual))

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() +
  geom_point() +
  coord_fixed()
```

```{r, fig.width = 3, fig.height = 2}

pr <- arrange(pr, desc(prediction))

rocPoints <- data.frame(fpr = cumsum(!pr$actual) / sum(!pr$actual),
                        tpr = cumsum(pr$actual) / sum(pr$actual))

ggplot(rocPoints, aes(x = fpr, y = tpr)) +
  geom_step() +
  geom_point()
```


## Area under the curve (AUC)

### Discrimination performance

- introduce slider! Watch ROC curve change.
- emphasise how only *ordering* is important.

### Discrimination vs. calibration

- show extreme examples with all berries bunched up at one end of the scale.
- consider animation where berries jiggle arround, but don't swap places.
- Frank Harrel quote

## Optimal decision theory

- Mostly provide links to resources


## Closing

< visualisation that ties everything together >


<script src = "/js/d3.js"></script>
<script src = "/js/roc/confusion-matrix.js"></script>
<script src = "/js/roc/berry-cluster-view.js"></script>
<script src = "/js/roc/roc-point.js"></script>
<script src = "/js/roc/continuous-output-plot.js"></script>
<script src = "/js/roc/discrim-plot.js"></script>
<script src = "/js/roc/roc-visuals.js"></script>
